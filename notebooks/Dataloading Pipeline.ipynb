{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict, Any, NoReturn, Tuple, Optional, Union\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, SequentialSampler, \n",
    "                              Dataset, TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System adjustments - for all colums to fit into output (default width is 80)\n",
    "pd.options.display.width = 2500\n",
    "pd.options.display.max_rows = 999\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train-test in training mode\n",
    "GENERAL_DATA_DIR = Path('D:\\\\Data\\\\EyesSimulation Sessions\\\\Export_full')\n",
    "INITIAL_DATA_DIR = GENERAL_DATA_DIR / \"Export_full\"\n",
    "TEST_SEEN_DATA_DIR = GENERAL_DATA_DIR / \"test_seen\"\n",
    "TEST_UNSEEN_DATA_DIR = GENERAL_DATA_DIR / \"test_unseen\"\n",
    "TRAIN_DATA_DIR = GENERAL_DATA_DIR / \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_session(data: pd.DataFrame,\n",
    "                    filter_threshold: int=50) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Group data by sessions.\n",
    "    :param data: single DataFrame with all recorded sessions\n",
    "    :param filter_threshold: minimum length of session recording to select\n",
    "    :return: list of sessions (as DataFrames).\n",
    "    \"\"\"\n",
    "    sess_data = []\n",
    "    for group_name, group in data.groupby(by=['user_id', 'session_id']):\n",
    "        group['user_id'] = group_name[0]\n",
    "        group['session_id'] = group_name[1]\n",
    "        if group.shape[0] > filter_threshold:\n",
    "            sess_data.append(group)\n",
    "    print(f\"Resulted list of data length: {len(sess_data)}\")\n",
    "    del data\n",
    "    return sess_data\n",
    "\n",
    "\n",
    "def horizontal_align_data(df: pd.DataFrame,\n",
    "                          grouping_cols: Union[str, List[str]],\n",
    "                          aligning_cols: List[str]) -> pd.DataFrame:\n",
    "    if len(aligning_cols) != 2:\n",
    "        print(\"Should be given two separate columns of coordinates.\")\n",
    "        return df\n",
    "\n",
    "    hdf = []\n",
    "    for group_name, group_df in tqdm(df.groupby(by=grouping_cols)):\n",
    "        group_df = pd.DataFrame(group_df[aligning_cols].T.values.flatten(order='F').reshape(1, -1),\n",
    "                                columns=list(chain.from_iterable([[col_name + str(col_n) for col_name in [\"x_\", \"y_\"]]\n",
    "                                                                  for col_n in range(group_df.shape[0])])))\n",
    "        for i, col_name in enumerate(grouping_cols):\n",
    "            group_df[col_name] = group_name[i]\n",
    "        hdf.append(group_df)\n",
    "    hdf = pd.concat(hdf, axis=0)\n",
    "    return hdf\n",
    "\n",
    "\n",
    "def vertical_align_data(data: pd.DataFrame,\n",
    "                        data_col: Union[str, List[str]],\n",
    "                        target_col: str,\n",
    "                        guid_col: str) -> pd.DataFrame:\n",
    "    # Transforms to long forme DF\n",
    "    ts_df = []\n",
    "    for i, row in data.iterrows():\n",
    "        df = pd.DataFrame(columns=['x', 'y', 'label', 'guid'])\n",
    "        if type(data_col) == str:\n",
    "            # Joined array of x and y\n",
    "            df['x'] = row[data_col].reshape(-1, 2)[:, 0]\n",
    "            df['y'] = row[data_col].reshape(-1, 2)[:, 1]\n",
    "        else:\n",
    "            # Separately x and y\n",
    "            df['x'] = row[data_col[0]]\n",
    "            df['y'] = row[data_col[1]]\n",
    "        df['label'] = row[target_col]\n",
    "        df['guid'] = row[guid_col]\n",
    "        ts_df.append(df)\n",
    "\n",
    "    data = pd.concat(ts_df).reset_index().rename({\"index\": \"i\"}, axis=1)\n",
    "    data.label = data.label.astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset: pd.DataFrame, label_col_name: str,\n",
    "                  max_seq_len: int):\n",
    "    data = []\n",
    "    guid_cnt = 0\n",
    "    for i, (label, xy) in tqdm(enumerate(zip(dataset[label_col_name].values,\n",
    "                                                      dataset.filter(regex=(\"[\\d]+\")).values))):\n",
    "        xy = xy[~np.isnan(xy)]  # not nan values\n",
    "\n",
    "        if len(xy) >= max_seq_len:\n",
    "            for i in range(len(xy) // (max_seq_len)):\n",
    "                if len(xy[i * max_seq_len: (i + 1) * max_seq_len]) > 0.85 * max_seq_len:\n",
    "                    guid_cnt += 1\n",
    "                    data.append({\"guid\": guid_cnt,\n",
    "                                 \"data\": xy[i * max_seq_len: (i + 1) * max_seq_len],\n",
    "                                 \"label\": label})\n",
    "        elif len(xy) > 0.85 * max_seq_len:\n",
    "            guid_cnt += 1\n",
    "            data.append({'guid': guid_cnt,\n",
    "                         'data': xy,\n",
    "                         'label': label})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def pad_dataset(data: List[Dict[str, Any]], max_seq_len: int,\n",
    "                pad_symbol: float):\n",
    "    ret_data = []\n",
    "    try:\n",
    "        for _ in range(len(data)):\n",
    "            data_pair = data.pop()\n",
    "            if len(data_pair['data']) < max_seq_len:\n",
    "                ret_data.append({'guid': data_pair['guid'],\n",
    "                                 'data': np.pad(data_pair['data'],\n",
    "                                                pad_width=(0, max_seq_len - len(data_pair['data'])),\n",
    "                                                mode='constant', constant_values=0.0),\n",
    "                                 'label': data_pair['label']})\n",
    "            else:\n",
    "                ret_data.append(data_pair)\n",
    "    except:\n",
    "        print(\"Data list ended.\")\n",
    "    del data\n",
    "    return ret_data\n",
    "\n",
    "\n",
    "def truncate_dataset(data: List[Dict[str, Any]], max_seq_len: int):\n",
    "    ret_data = []\n",
    "    try:\n",
    "        for _ in range(len(data)):\n",
    "            data_pair = data.pop()\n",
    "            if len(data_pair['data']) > max_seq_len:\n",
    "                ret_data.append({'guid': data_pair['guid'],\n",
    "                                 'data': data_pair['data'][:max_seq_len],\n",
    "                                 'label': data_pair['label']})\n",
    "            else:\n",
    "                ret_data.append(data_pair)\n",
    "    except:\n",
    "        print(\"Data list ended.\")\n",
    "    del data\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionsDataset(Dataset):\n",
    "    \"\"\"Eye Gaze Sessions dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, targets_fn: str, root_dir: str, \n",
    "                 do_transform: bool=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self._targets_mapping = pd.read_csv(targets_fn, sep=\";\", encoding=\"utf-8\")\n",
    "        self._data_root_dir = root_dir\n",
    "        self._sess_fns = random.shuffle([fn for fn in glob.glob(root_dir + \"\\\\*.csv\") if not fn.endswith(\"_affmatrix.csv\")])\n",
    "        self._do_transform = do_transform\n",
    "        self._transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._sess_fns)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sess_fn = self._sess_fns[idx]\n",
    "        sess = pd.read_csv(sess_fn, sep='\\t')\n",
    "        target = self._targets_mapping.loc[self._targets_mapping.session_filename == sess_fn].user_id\n",
    "        sample = {'sess': sess, 'target': target}\n",
    "        \n",
    "        if self._do_transform:\n",
    "            sample = self._transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "class EyeMovementsClassification(object):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        # Parameters\n",
    "        self._filter_params = dict(read_json(config.get(\"EyemovementClassification\", \"filtering_params\")))\n",
    "        self._model_params = dict(read_json(config.get('EyemovementClassification', 'model_params')))\n",
    "        \n",
    "        # Classification model\n",
    "        ivdt = IVDT(saccade_min_velocity=model_params.get('saccade_min_velocity'),\n",
    "                    saccade_min_duration=model_params.get('min_saccade_duration_threshold'),\n",
    "                    saccade_max_duration=model_params.get('max_saccade_duration_threshold'),\n",
    "                    window_size=model_params.get('window_size'),\n",
    "                    dispersion_threshold=model_params.get('dispersion_threshold'))\n",
    "\n",
    "        # Post filtering\n",
    "        thresholds_dict = {'min_saccade_duration_threshold': model_params.get('min_saccade_duration_threshold'),\n",
    "                           'max_saccade_duration_threshold': model_params.get('max_saccade_duration_threshold'),\n",
    "                           'min_fixation_duration_threshold': model_params.get('min_fixation_duration_threshold'),\n",
    "                           'min_sp_duration_threshold': model_params.get('min_sp_duration_threshold')}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data = groupby_session(sample)\n",
    "        data = sgolay_filter_dataset(data, **dict(read_json(config.get(\"EyemovementClassification\",\n",
    "                                                     \"filtering_params\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
